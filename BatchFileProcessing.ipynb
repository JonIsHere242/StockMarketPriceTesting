{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##===================(PRICE DATA DOWNLOADER)===================##\n",
    "##  This script downloads price data from yahoo finance for all tickers in a file avalable on the SEC website\n",
    "## The file is available at https://www.sec.gov/include/ticker.txt\n",
    "## After downloding all the data it will make indicators and then plot them against the percent chaneg in price \n",
    "##==============================================================##\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install scipy\n",
    "!pip install yfinance\n",
    "!pip install plotly\n",
    "!pip install tqdm\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import yfinance as yf\n",
    "import scipy.stats as stats\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "from tqdm.notebook import tqdm\n",
    "from random import uniform\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.stats import linregress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##===================(PRICE DATA DOWNLOADER)===================##\n",
    "##Roughly 11895 tickers with a download time of 1.05 seconds per ticker\n",
    "##Need to add an updated ticker list that only uses good tickers for week day data downloads and a seperate list for weekend downloads\n",
    "##Estimated time to run 3:30:00 ~ 3 hours 30 minutes\n",
    "\n",
    "def read_tickers(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        tickers = [line.split('\\t')[0] for line in file.readlines()]\n",
    "    return tickers\n",
    "\n",
    "def fetch_and_save_data(tickers, output_dir):\n",
    "    last_call_time = None\n",
    "    with tqdm(total=len(tickers), desc='Fetching Data') as pbar:\n",
    "        for ticker in tickers:\n",
    "            if last_call_time is not None:\n",
    "                elapsed = time.time() - last_call_time\n",
    "                if elapsed < 1.05:\n",
    "                    time.sleep(1.05 - elapsed)\n",
    "            \n",
    "            data = yf.download(ticker, interval='1d', period=\"max\", progress=False)\n",
    "            filename = os.path.join(output_dir, f'{ticker}.csv')\n",
    "            data.to_csv(filename)\n",
    "            last_call_time = time.time()\n",
    "            pbar.update(1)\n",
    "\n",
    "file_path = 'TickersCIK.txt'\n",
    "output_dir = 'Data/DailyPriceData'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "tickers = read_tickers(file_path)\n",
    "fetch_and_save_data(tickers, output_dir)\n",
    "print(f'Completed: {len(os.listdir(output_dir))} out of {len(tickers)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d75833d1aa9942ae8a7280d7b4e3f9ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cleaning Data:   0%|          | 0/8236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files processed: 8236\n",
      "Total files removed: 2339\n",
      "Percentage of files removed: 28.40%\n",
      "Mean closing price of removed files: 76.09\n"
     ]
    }
   ],
   "source": [
    "##===================(PRICE DATA CLEANER)===================##\n",
    "##Estimated Time to run 0:00:30 ~ 30 seconds\n",
    "\n",
    "def clean_price_data(folder_path):\n",
    "    \"\"\"\n",
    "    Cleans stock price data files based on specified criteria and provides diagnostics.\n",
    "\n",
    "    Args:\n",
    "    folder_path (str): Path to the folder containing stock price data files.\n",
    "\n",
    "    Returns:\n",
    "    None: Files are cleaned, and diagnostics are printed.\n",
    "    \"\"\"\n",
    "    total_files = 0\n",
    "    removed_files = 0\n",
    "    filetooshort = 0\n",
    "    removed_files_mean_close = []\n",
    "\n",
    "    for file_name in tqdm(os.listdir(folder_path), desc='Cleaning Data'):\n",
    "        total_files += 1\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        # Read data\n",
    "        try:\n",
    "            data = pd.read_csv(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Check criteria\n",
    "        remove_file = False\n",
    "\n",
    "        # Criteria 1: Remove if any closing price is greater than 100,000\n",
    "        if data['Close'].max() > 100000:\n",
    "            remove_file = True\n",
    "\n",
    "        # Criteria 2: Remove if closing prices are below 0 for more than 5 rows\n",
    "        if (data['Close'] < 0).sum() > 5:\n",
    "            remove_file = True\n",
    "\n",
    "        # Criteria 3: Remove if more than 1% NaN values\n",
    "        if data.isnull().mean().mean() > 0.01:\n",
    "            remove_file = True\n",
    "\n",
    "        # Criteria 4: Remove if less than 500 days of data\n",
    "        if len(data) < 500:\n",
    "            remove_file = True\n",
    "            filetooshort += 1\n",
    "\n",
    "        ## Criteria 5: Remove if the closing price is less than 0.01 for more than 1/3 of the data\n",
    "        if (data['Close'] < 0.01).sum() > len(data)/3:\n",
    "            remove_file = True\n",
    "\n",
    "        ## Criteria 6: remove if the date does not extend to 2022\n",
    "        if data['Date'].max() < '2022-01-01':\n",
    "            remove_file = True\n",
    "            \n",
    "        # Criteria 7: Remove if the volume * the closing price is less than 10,000 for more than 1/3 of the data\n",
    "        if (data['Close'] * data['Volume'] < 10000).sum() > len(data)/3:\n",
    "            remove_file = True\n",
    "\n",
    "        # Process file removal\n",
    "        if remove_file:\n",
    "            removed_files += 1\n",
    "            removed_files_mean_close.append(data['Close'].mean())\n",
    "            os.remove(file_path)\n",
    "\n",
    "    # Display diagnostics\n",
    "    if total_files > 0:\n",
    "        print(f\"Total files processed: {total_files}\")\n",
    "        print(f\"Total files removed: {removed_files}\")\n",
    "        print(f\"Percentage of files removed: {removed_files / total_files * 100:.2f}%\")\n",
    "        if removed_files > 0:\n",
    "            print(f\"Mean closing price of removed files: {sum(removed_files_mean_close) / removed_files:.2f}\")\n",
    "    else:\n",
    "        print(\"No files to process.\")\n",
    "\n",
    "# Use the function\n",
    "clean_price_data('Data\\DailyPriceData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80abe0e70b57469994024a88fb788aa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run complex indicator function: 0.07 seconds\n",
      "Time to run complex indicator function: 0.07 seconds\n",
      "Time to run complex indicator function: 0.03 seconds\n",
      "Time to run complex indicator function: 0.07 seconds\n",
      "Time to run complex indicator function: 0.04 seconds\n",
      "Time to run complex indicator function: 0.02 seconds\n",
      "Time to run complex indicator function: 0.03 seconds\n",
      "Time to run complex indicator function: 0.07 seconds\n",
      "Time to run complex indicator function: 0.02 seconds\n",
      "Time to run complex indicator function: 0.07 seconds\n",
      "Time to run complex indicator function: 0.02 seconds\n",
      "Time to run complex indicator function: 0.05 seconds\n",
      "Time to run complex indicator function: 0.07 seconds\n",
      "Time to run complex indicator function: 0.08 seconds\n",
      "Time to run complex indicator function: 0.07 seconds\n",
      "Time to run complex indicator function: 0.06 seconds\n",
      "Time to run complex indicator function: 0.07 seconds\n",
      "Time to run complex indicator function: 0.06 seconds\n",
      "Time to run complex indicator function: 0.07 seconds\n",
      "Time to run complex indicator function: 0.02 seconds\n",
      "Time to run complex indicator function: 0.07 seconds\n",
      "Time to run complex indicator function: 0.06 seconds\n",
      "Time to run complex indicator function: 0.07 seconds\n",
      "Time to run complex indicator function: 0.03 seconds\n",
      "Time to run complex indicator function: 0.07 seconds\n",
      "Time to run complex indicator function: 0.08 seconds\n",
      "Time to run complex indicator function: 0.07 seconds\n",
      "Time to run complex indicator function: 0.08 seconds\n",
      "Time to run complex indicator function: 0.02 seconds\n",
      "Time to run complex indicator function: 0.02 seconds\n",
      "Time to run complex indicator function: 0.02 seconds\n",
      "Time to run complex indicator function: 0.02 seconds\n",
      "Time to run complex indicator function: 0.02 seconds\n",
      "Time to run complex indicator function: 0.07 seconds\n",
      "Time to run complex indicator function: 0.02 seconds\n",
      "Time to run complex indicator function: 0.07 seconds\n",
      "Time to run complex indicator function: 0.07 seconds\n",
      "Time to run complex indicator function: 0.04 seconds\n",
      "Time to run complex indicator function: 0.03 seconds\n",
      "Time to run complex indicator function: 0.07 seconds\n",
      "Time to run complex indicator function: 0.05 seconds\n",
      "Time to run complex indicator function: 0.02 seconds\n",
      "Time to run complex indicator function: 0.07 seconds\n",
      "Time to run complex indicator function: 0.26 seconds\n",
      "Time to run complex indicator function: 0.04 seconds\n",
      "Time to run complex indicator function: 0.12 seconds\n",
      "Time to run complex indicator function: 0.04 seconds\n",
      "Time to run complex indicator function: 0.07 seconds\n",
      "Time to run complex indicator function: 0.07 seconds\n",
      "Time to run complex indicator function: 0.02 seconds\n"
     ]
    }
   ],
   "source": [
    "##========================(INDICATOR CALCULATOR)========================##\n",
    "#Estimated time to run 00:45:00 ~ 45 minutes\n",
    "#Target time to run 0:10:00 ~ 10 minutes\n",
    "\n",
    "def calculate_slope(series):\n",
    "    if series.isnull().all() or len(series.dropna()) <= 1:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return stats.linregress(range(len(series)), series.values)[0]\n",
    "\n",
    "def indicators(df):\n",
    "    close = df['Close']\n",
    "    high = df['High']\n",
    "    low = df['Low']\n",
    "    volume = df['Volume']\n",
    "    close_shift_1 = close.shift(1)\n",
    "    true_range = np.maximum(high - low, np.maximum(np.abs(high - close_shift_1), np.abs(close_shift_1 - low)))\n",
    "    df['ATR'] = true_range.rolling(window=14).mean() \n",
    "    df['ATR%'] = (df['ATR'] / close) * 100\n",
    "    df['200ma'] = close.rolling(window=200).mean()\n",
    "    df['14ma'] = close.rolling(window=14).mean()\n",
    "    df['14ma%'] = ((close - df['14ma']) / df['14ma']) * 100\n",
    "    df['200ma%'] = ((close - df['200ma']) / df['200ma']) * 100\n",
    "    df['14ma-200ma'] = df['14ma'] - df['200ma']\n",
    "    df['14ma%_change'] = df['14ma%'].pct_change()\n",
    "    df['14ma%_count'] = df['14ma%'].rolling(window=14).apply(lambda x: np.sum(x > 0))\n",
    "    df['200ma%_count'] = df['200ma%'].rolling(window=200).apply(lambda x: np.sum(x > 0))\n",
    "    df['14ma_crossover'] = (close > df['14ma']).astype(int)\n",
    "    df['200ma_crossover'] = (close > df['200ma']).astype(int)\n",
    "    df['200DAY_ATR'] = df['200ma'] + df['ATR']\n",
    "    df['200DAY_ATR-'] = df['200ma'] - df['ATR']\n",
    "    df['200DAY_ATR%'] = df['200DAY_ATR'] / close\n",
    "    df['200DAY_ATR-%'] = df['200DAY_ATR-'] / close\n",
    "    df['percent_from_high'] = ((close - close.cummax()) / close.cummax()) * 100\n",
    "    df['new_high'] = (close == close.cummax())\n",
    "    df['days_since_high'] = (~df['new_high']).cumsum() - (~df['new_high']).cumsum().where(df['new_high']).ffill().fillna(0)\n",
    "    df['percent_range'] = (high - low) / close * 100\n",
    "    typical_price = (high + low + close) / 3\n",
    "    df['VWAP'] = (typical_price * volume).rolling(window=14).sum() / volume.rolling(window=14).sum()\n",
    "    df['VWAP_std14'] = df['VWAP'].rolling(window=14).std()\n",
    "    df['VWAP_std200'] = df['VWAP'].rolling(window=20).std()\n",
    "    df['VWAP%'] = ((close - df['VWAP']) / df['VWAP']) * 100\n",
    "    df['VWAP%_from_high'] = ((df['VWAP'] - close.cummax()) / close.cummax()) * 100\n",
    "    obv_condition = df['Close'] > close_shift_1\n",
    "    df['OBV'] = np.where(obv_condition, volume, np.where(~obv_condition, -volume, 0)).cumsum()\n",
    "    df['OBV_change'] = df['OBV'].diff()\n",
    "    df['Close_change'] = close.diff()\n",
    "    df['OBV_change%'] = df['OBV_change'] / df['OBV']\n",
    "    df['OBV_change%_std'] = df['OBV_change%'].rolling(window=14).std()\n",
    "    df['OBV_change%_std>1'] = (df['OBV_change%_std'] > 1).astype(int)\n",
    "    df['OBV_change_std'] = df['OBV_change'].rolling(window=14).std()\n",
    "    df['OBV_change_std>1'] = (df['OBV_change_std'] > 1).astype(int)\n",
    "    df['OBV_Divergence'] = (np.sign(df['OBV_change']) != np.sign(df['Close_change'])) & (df['OBV_change'].abs() > df['OBV'].rolling(window=14).std())\n",
    "    df = df.round(8)\n",
    "    df = calculate_complex_indicators(df)\n",
    "    df = df.round(8)\n",
    "    return df\n",
    "\n",
    "\n",
    "def MinMaxSlope(df, window_size=720):\n",
    "    df = df.reset_index(drop=True)\n",
    "    local_min = df['Close'].rolling(window=window_size, min_periods=1).min()\n",
    "    local_max = df['Close'].rolling(window=window_size, min_periods=1).max()\n",
    "    min_indices = df['Close'] == local_min\n",
    "    max_indices = df['Close'] == local_max\n",
    "    df['days_since_local_min'] = (~min_indices).cumsum()\n",
    "    df['days_since_local_max'] = (~max_indices).cumsum()\n",
    "    df['days_since_local_min'] = df['days_since_local_min'] - df['days_since_local_min'][min_indices].reindex(df.index, method='ffill').fillna(0)\n",
    "    df['days_since_local_max'] = df['days_since_local_max'] - df['days_since_local_max'][max_indices].reindex(df.index, method='ffill').fillna(0)\n",
    "    df['slope_min_max'] = np.nan\n",
    "\n",
    "    for idx in np.where(min_indices & (df['days_since_local_max'] <= window_size))[0]:\n",
    "        if idx - int(df.at[idx, 'days_since_local_max']) >= 0:\n",
    "            min_idx = idx - int(df.at[idx, 'days_since_local_max'])\n",
    "            max_idx = idx\n",
    "            delta_y = df.at[max_idx, 'Close'] - df.at[min_idx, 'Close']\n",
    "            delta_x = max_idx - min_idx\n",
    "            df.at[min_idx, 'slope_min_max'] = delta_y / delta_x if delta_x != 0 else np.nan\n",
    "\n",
    "    return df\n",
    "\n",
    "def find_local_extrema(df, column='Close', window_size=90):\n",
    "    \"\"\"\n",
    "    Find local maxima and minima (extrema) over a rolling window.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): DataFrame with stock data.\n",
    "    column (str): Column name to find extrema.\n",
    "    window_size (int): Window size for rolling calculation.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame with local maxima and minima indices.\n",
    "    \"\"\"\n",
    "    local_maxima = find_peaks(df[column], distance=window_size)[0]\n",
    "    local_minima = find_peaks(-df[column], distance=window_size)[0]\n",
    "    df['Local_Max'] = 0\n",
    "    df['Local_Min'] = 0\n",
    "    df.loc[local_maxima, 'Local_Max'] = 1\n",
    "    df.loc[local_minima, 'Local_Min'] = 1\n",
    "\n",
    "    return df\n",
    "\n",
    "def best_fit_lines(df, column='Close'):\n",
    "    \"\"\"\n",
    "    Calculate best fit lines through local extrema.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): DataFrame with local extrema information.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame with slope and intercept of best fit lines.\n",
    "    \"\"\"\n",
    "    max_indices = df.index[df['Local_Max'] == 1]\n",
    "    max_values = df.loc[max_indices, column]\n",
    "    slope_max, intercept_max, _, _, _ = linregress(max_indices, max_values)\n",
    "    min_indices = df.index[df['Local_Min'] == 1]\n",
    "    min_values = df.loc[min_indices, column]\n",
    "    slope_min, intercept_min, _, _, _ = linregress(min_indices, min_values)\n",
    "    return slope_max, intercept_max, slope_min, intercept_min\n",
    "\n",
    "def calculate_channel_metrics(df, slope_max, intercept_max, slope_min, intercept_min, column='Close'):\n",
    "    \"\"\"\n",
    "    Calculate channel metrics and detect channel breakouts.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): DataFrame with stock data.\n",
    "    slope_max (float): Slope of the best fit line for local maxima.\n",
    "    intercept_max (float): Intercept of the best fit line for local maxima.\n",
    "    slope_min (float): Slope of the best fit line for local minima.\n",
    "    intercept_min (float): Intercept of the best fit line for local minima.\n",
    "    column (str): Column name to use for breakout detection.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Updated DataFrame with channel metrics and breakout information.\n",
    "    \"\"\"\n",
    "    # Calculate channel lines\n",
    "    df['Top_Channel_Line'] = slope_max * df.index + intercept_max\n",
    "    df['Bottom_Channel_Line'] = slope_min * df.index + intercept_min\n",
    "    df['In_Channel'] = ((df[column] <= df['Top_Channel_Line']) & (df[column] >= df['Bottom_Channel_Line'])).astype(int)\n",
    "    df['Channel_Duration'] = df['In_Channel'].cumsum()\n",
    "    df['Average_Slope'] = (slope_max + slope_min) / 2\n",
    "    df['Breakout_Above'] = (df[column] > df['Top_Channel_Line']).astype(int)\n",
    "    df['Breakout_Below'] = (df[column] < df['Bottom_Channel_Line']).astype(int)\n",
    "    df['Breakout_Magnitude'] = np.where(df['Breakout_Above'], df[column] - df['Top_Channel_Line'], \n",
    "        np.where(df['Breakout_Below'], df['Bottom_Channel_Line'] - df[column], 0))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def calculate_complex_indicators(df):\n",
    "    timer = time.time()\n",
    "    df = MinMaxSlope(df)\n",
    "    \n",
    "    close = df['Close']\n",
    "    high = df['High']\n",
    "    low = df['Low']\n",
    "\n",
    "    close_shift_1 = close.shift(1)\n",
    "    low_shift_1 = low.shift(1)\n",
    "    high_shift_1 = high.shift(1)\n",
    "\n",
    "    df['percent_change_Close'] = df['Close'].pct_change()\n",
    "    df['pct_change_std'] = df['percent_change_Close'].rolling(window=20).std()\n",
    "    for shift in range(1, 4):\n",
    "        df[f'percent_change_Close_shift_{shift}'] = df['percent_change_Close'].shift(shift)\n",
    "\n",
    "    df['percent_change_Close_7'] = df['Close'].pct_change(periods=7)\n",
    "    df['percent_change_Close_30'] = df['Close'].pct_change(periods=30)\n",
    "    df['percent_change_Close>std'] = (df['percent_change_Close'] > df['pct_change_std']).astype(int)\n",
    "    df['percent_change_Close<std'] = (df['percent_change_Close'] < df['pct_change_std']).astype(int)\n",
    "    df['pct_change_std_rolling'] = df['pct_change_std'].rolling(window=14).mean()\n",
    "    df['pct_change_std_rolling'] = df['pct_change_std'].rolling(window=60).mean()\n",
    "\n",
    "    df['percent_change_Close>0'] = (df['percent_change_Close'] > 0).astype(int)\n",
    "    df['percent_change_Close<0'] = (df['percent_change_Close'] < 0).astype(int)\n",
    "    df['percent_change_Close>0_count'] = df['percent_change_Close>0'].rolling(window=14).sum()\n",
    "    df['percent_change_Close<0_count'] = df['percent_change_Close<0'].rolling(window=14).sum()\n",
    "    df['abs_pct_change_Close_7'] = df['percent_change_Close'].abs().rolling(window=7).mean()\n",
    "    df['abs_pct_change_Close_30'] = df['percent_change_Close'].abs().rolling(window=30).mean()\n",
    "    df['abs_pct_change_Close_7>abs_pct_change_Close_30'] = (df['abs_pct_change_Close_7'] > df['abs_pct_change_Close_30']).astype(int)\n",
    "    df['abs_pct_change_Close_7<abs_pct_change_Close_30'] = (df['abs_pct_change_Close_7'] < df['abs_pct_change_Close_30']).astype(int)\n",
    "\n",
    "    pct_change_close = df['percent_change_Close']\n",
    "    close_shift_1 = df['Close'].shift(1)\n",
    "    low_shift_1 = df['Low'].shift(1)\n",
    "    high_shift_1 = df['High'].shift(1)\n",
    "    threshold_multiplier = 0.65\n",
    "    abnormal_pct_change_threshold = pct_change_close.rolling(window=20).mean() + threshold_multiplier * df['pct_change_std']\n",
    "    df['days_since_abnormal_pct_change'] = (pct_change_close > abnormal_pct_change_threshold).cumsum()\n",
    "    significant_figures = [10, 100, 500, 1000]\n",
    "    custom_values = [50,69,420,666,777,888,999]\n",
    "    multiples = [x for figure in significant_figures for x in range(figure, int(df['Close'].max()) + 1, figure)]\n",
    "    psych_levels = set(multiples + custom_values)\n",
    "    rolling_min = df['Close'].rolling(window=500).min()\n",
    "    df['psych_level'] = df['Close'].apply(lambda x: any(abs(x - level) / level <= 0.01 for level in psych_levels))\n",
    "    df['psych_level'] |= (abs(df['Close'] - rolling_min) / rolling_min <= 0.01)\n",
    "    df['psych_level%'] = (df['Close'] - rolling_min) / rolling_min * 100\n",
    "    df['days_since_psych_level'] = (~df['psych_level']).cumsum() - (~df['psych_level']).cumsum().where(df['psych_level']).ffill().fillna(0)\n",
    "    df['direction_flipper'] = (pct_change_close > 0).astype(int)\n",
    "    df['direction_flipper_count5'] = df['direction_flipper'].rolling(window=5).sum()\n",
    "    df['direction_flipper_count_10'] = df['direction_flipper'].rolling(window=10).sum()\n",
    "    df['direction_flipper_count_14'] = df['direction_flipper'].rolling(window=14).sum()\n",
    "    df['ATR'] = df['Close'].rolling(window=14).apply(lambda x: np.mean(np.abs(np.diff(x))))\n",
    "    keltner_central = df['Close'].ewm(span=20).mean()\n",
    "    keltner_range = df['ATR'] * 1.5\n",
    "    df['KC_UPPER%'] = ((keltner_central + keltner_range) - df['Close']) / df['Close'] * 100\n",
    "    df['KC_LOWER%'] = (df['Close'] - (keltner_central - keltner_range)) / df['Close'] * 100\n",
    "    delta = df['Close'].diff()\n",
    "    gain = delta.clip(lower=0).rolling(window=14).mean()\n",
    "    loss = (-delta).clip(lower=0).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    df['RSI'] = 100 - (100 / (1 + rs))\n",
    "    rolling_mean = df['Close'].rolling(window=20).mean()\n",
    "    rolling_std = df['Close'].rolling(window=20).std()\n",
    "    df['B%'] = (df['Close'] - (rolling_mean - 2 * rolling_std)) / (4 * rolling_std)\n",
    "    positive_streak = (pct_change_close > 0).astype(int).cumsum()\n",
    "    negative_streak = (pct_change_close < 0).astype(int).cumsum()\n",
    "    df['positive_streak'] = positive_streak - positive_streak.where(pct_change_close <= 0).ffill().fillna(0)\n",
    "    df['negative_streak'] = negative_streak - negative_streak.where(pct_change_close >= 0).ffill().fillna(0)\n",
    "    gap_threshold_percent = 0.5 if pct_change_close.std() > 1 else 0\n",
    "    df['is_gap_up'] = (df['Low'] - high_shift_1) / close_shift_1 * 100 > gap_threshold_percent\n",
    "    df['is_gap_down'] = (df['High'] - low_shift_1) / close_shift_1 * 100 < -gap_threshold_percent\n",
    "    df['move_from_gap%'] = np.where(df['is_gap_up'], (df['Close'] - low_shift_1) / low_shift_1 * 100,\n",
    "        np.where(df['is_gap_down'], (df['Close'] - high_shift_1) / high_shift_1 * 100, 0))\n",
    "    df['VPT'] = df['Volume'] * ((df['Close'] - df['Close'].shift(1)) / df['Close'].shift(1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    df = find_local_extrema(df, column='Close', window_size=20)\n",
    "    slope_max, intercept_max, slope_min, intercept_min = best_fit_lines(df)\n",
    "    df = calculate_channel_metrics(df, slope_max, intercept_max, slope_min, intercept_min, column='Close')\n",
    "    df = df.drop(['Open', 'High', 'Low', 'Close', 'Adj Close', 'VWAP', '200DAY_ATR-', '200DAY_ATR', 'Close_change', 'Volume', 'ATR', 'OBV', '200ma', '14ma'], axis=1)\n",
    "    timer2 = time.time()\n",
    "    print(f\"Time to run complex indicator function: {round(timer2 - timer, 3)} seconds\")\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##===================(CONTROL LOGIC)===================##\n",
    "##===================(CONTROL LOGIC)===================##\n",
    "##===================(CONTROL LOGIC)===================##\n",
    "\n",
    "\n",
    "def IntermediaiteDataProcessing(df):\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df = df.replace([True, False], [1, 0])\n",
    "    df = df.bfill()\n",
    "    df = df.ffill()    \n",
    "    df = df.iloc[int(len(df) * 0.1):]    \n",
    "    return df\n",
    "\n",
    "\n",
    "def squash_col_outliers(df, col_name=None, num_std_dev=3):\n",
    "    columns_to_process = [col_name] if col_name is not None else df.select_dtypes(include=['float64']).columns\n",
    "    for col in columns_to_process:\n",
    "        if col not in df.columns or df[col].dtype != 'float64':\n",
    "            continue  # Skip non-existent or non-float columns\n",
    "        mean = df[df[col] != 0][col].mean()\n",
    "        std_dev = df[df[col] != 0][col].std()\n",
    "        lower_bound = mean - num_std_dev * std_dev\n",
    "        upper_bound = mean + num_std_dev * std_dev\n",
    "        num_lower_clipped = (df[col] < lower_bound).sum()\n",
    "        num_upper_clipped = (df[col] > upper_bound).sum()\n",
    "        df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "        print(f\"Column '{col}': Clipped {num_lower_clipped} lower outliers and {num_upper_clipped} upper outliers.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def scale_column(col, precision_type='float32'):\n",
    "    if col.dtype == 'bool':\n",
    "        return col\n",
    "    if col.mean() > 3:\n",
    "        col = col.apply(lambda x: x + uniform(-0.001, 0.001) if x not in [0, 1] else x)\n",
    "    \n",
    "    scaler = MinMaxScaler() if col.dtype in ['int64', 'int32'] else RobustScaler()\n",
    "    scaled_col = scaler.fit_transform(col.values.reshape(-1, 1))\n",
    "    return scaled_col.astype(precision_type)\n",
    "\n",
    "def scale_data(df, precision_type='float32'):\n",
    "    df = df.copy()\n",
    "    df.ffill(inplace=True)\n",
    "    df.bfill(inplace=True)\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    scalers = {}\n",
    "\n",
    "    for col_name in df.select_dtypes(include=['number']).columns:\n",
    "        if df[col_name].nunique() == 2 and sorted(df[col_name].unique()) == [0, 1]:\n",
    "            continue\n",
    "        scaled_col = scale_column(df[col_name], precision_type)\n",
    "        df[col_name] = scaled_col\n",
    "        if df[col_name].sum() == 0:\n",
    "            continue\n",
    "        scaler = MinMaxScaler() if df[col_name].dtype in ['int64', 'int32'] else RobustScaler()\n",
    "        scaler.fit(df[col_name].values.reshape(-1, 1))\n",
    "        scalers[f'scaler_{col_name}'] = scaler\n",
    "    return df, scalers\n",
    "\n",
    "\n",
    "def process_stock_data(file_path, output_dir, Testing):\n",
    "    \"\"\"\n",
    "    Reads a CSV file, computes indicators, scales the data, and saves it to a new directory.\n",
    "    Skips files with fewer than `min_rows` rows of data.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df.tail(4000)\n",
    "    df = indicators(df)\n",
    "    if Testing == False:\n",
    "        df, _ = scale_data(df)\n",
    "    df = IntermediaiteDataProcessing(df)\n",
    "    df = squash_col_outliers(df, col_name='specific_column', num_std_dev=2)\n",
    "    output_file = os.path.join(output_dir, os.path.basename(file_path))\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "\n",
    "\n",
    "def process_files(input_dir, output_dir, Testing):\n",
    "    \"\"\"\n",
    "    Process all stock data files in the specified input directory and save them to the output directory.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Create output directory if it doesn't exist\n",
    "    files = [f for f in os.listdir(input_dir) if f.endswith('.csv')]\n",
    "    if Testing == True:\n",
    "        files = files[:50]\n",
    "    for file in tqdm(files, desc=\"Processing Files\"):\n",
    "        file_path = os.path.join(input_dir, file)\n",
    "        process_stock_data(file_path, output_dir,Testing)\n",
    "\n",
    "\n",
    "\n",
    "def main(Testing):\n",
    "    input_dir = 'Data/DailyPriceData'\n",
    "    output_dir = 'Data/ProcessedPriceData'\n",
    "    process_files(input_dir, output_dir, Testing)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    folder = 'Data/ProcessedPriceData'\n",
    "    for filename in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
    "\n",
    "\n",
    "    Testing = True\n",
    "    main(Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##===================(Data Testing/Improvment)===================##\n",
    "##if testing is true then only 50 files will be processed Estimated time to run 0:00:01 ~ 1 seconds\n",
    "##if testing is false then all files will be processed Estimated time to run 0:00:10 ~ 10 seconds\n",
    "\n",
    "columns_of_interest = ['percent_change_Close'] \n",
    "folder_path = 'Data\\ProcessedPriceData'  \n",
    "num_files_to_sample = 25\n",
    "output_csv_folder = 'Data\\Logging'\n",
    "\n",
    "def select_random_files(folder, num_files):\n",
    "    all_files = [f for f in os.listdir(folder) if f.endswith('.csv')]\n",
    "    return random.sample(all_files, min(num_files, len(all_files)))\n",
    "\n",
    "def calculate_correlations_for_column(df, column):\n",
    "    if column in df.columns:\n",
    "        if df[column].sum() == 0:\n",
    "            return None\n",
    "        numeric_df = df.select_dtypes(include=[np.number])  # Select only numeric columns\n",
    "        if len(numeric_df) > 0:\n",
    "            return numeric_df.corrwith(numeric_df[column]).drop(column)\n",
    "    return None\n",
    "\n",
    "def process_files_and_save(folder, files, columns, output_folder):\n",
    "    correlations = {col: pd.Series(dtype='float') for col in columns}\n",
    "    for file in files:\n",
    "        df = pd.read_csv(os.path.join(folder, file))\n",
    "        for col in columns:\n",
    "            corr_series = calculate_correlations_for_column(df, col)\n",
    "            if corr_series is not None:\n",
    "                correlations[col] = correlations[col].add(corr_series, fill_value=0)\n",
    "\n",
    "    for col, corr_series in correlations.items():\n",
    "        avg_corr = corr_series / len(files)\n",
    "        sorted_corr = avg_corr.sort_values(ascending=False).reset_index()\n",
    "        sorted_corr.columns = ['Column', 'Correlation']\n",
    "        sorted_corr.to_csv(os.path.join(output_folder, f'correlation_with_{col}.csv'), index=False)\n",
    "    plot_correlation(avg_corr, col)\n",
    "\n",
    "def plot_correlation(correlation_series, column_name):\n",
    "    sorted_corr = correlation_series.sort_values(ascending=False)\n",
    "\n",
    "    fig = px.bar(sorted_corr, \n",
    "                 labels={'index': 'Column', 'value': 'Correlation'},\n",
    "                 title=f'Average Correlation with {column_name}')\n",
    "    fig.update_layout(xaxis_title='Column', yaxis_title='Correlation')\n",
    "    fig.show()\n",
    "\n",
    "if len(os.listdir(folder_path)) > 100:\n",
    "    num_files_to_sample = int(len(os.listdir(folder_path)) / 3)\n",
    "\n",
    "selected_files = select_random_files(folder_path, num_files_to_sample)\n",
    "process_files_and_save(folder_path, selected_files, columns_of_interest, output_csv_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
